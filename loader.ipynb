{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from Confuse import ConfuseMatrix\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sys import stdout\n",
    "import time\n",
    "import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_normalize():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if img.dtype == torch.uint8:\n",
    "            img = img.to(torch.float32)\n",
    "            img /= 255\n",
    "        elif (img.dtype == np.uint8) or (img.dtype == np.uint16):\n",
    "            img = img.astype('float32')\n",
    "            img /= 255\n",
    "        return img\n",
    "    \n",
    "\n",
    "\n",
    "class OcularDataset(Dataset):\n",
    "\n",
    "    def __init__(self, datadir, csv, transforms = None):\n",
    "        self.csv = csv\n",
    "        self.datadir = datadir\n",
    "        self.transform = transforms\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.datadir,\n",
    "                                self.csv.iloc[idx, -1])\n",
    "        image = read_image(img_name)\n",
    "        label = self.csv.iloc[idx, -2]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "# device = 'cuda'\n",
    "\n",
    "## data loading\n",
    "datadir = 'ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/Training Images'\n",
    "csvdir = 'ocular-disease-recognition-odir5k/full_df.csv'\n",
    "csv = pd.read_csv(csvdir)\n",
    "temp = ([np.array(eval(csv.iloc[i, -2]), dtype = 'float32') for i in range(len(csv))])\n",
    "csv.target = temp\n",
    "batchsize = 100\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), img_normalize()])\n",
    "dataset = OcularDataset(datadir, csv, transform)\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [.7, .2, .1])\n",
    "num_workers = 6\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batchsize, shuffle=False, num_workers= num_workers, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batchsize, shuffle=False, num_workers = 0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batchsize, shuffle=False, num_workers = 0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.patchify = nn.Unfold(kernel_size = patch_size, stride = patch_size)\n",
    "        self.mlp = nn.Linear(in_channels * patch_size ** 2, embed_dim)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patchify(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        return F.relu(self.mlp(x))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head attention\n",
    "        x1 = self.norm1(x)\n",
    "        x2 = x + self.attn(x1, x1, x1)[0]\n",
    "        # MLP\n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = x2 + self.mlp(x3)\n",
    "        return x3\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000, embed_dim=768, num_heads=12, mlp_dim=3072, num_layers=12, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer Encoder Blocks\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Append class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through Transformer layers\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = torch.mean(x, dim = 1)\n",
    "        # print(x.shape)\n",
    "        # cls_token_final = x[:, 0]\n",
    "\n",
    "        # Classification head\n",
    "        x = F.sigmoid(self.head(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(max, iter, start_time, dash_len = 50, starter_txt = ''):\n",
    "    f = starter_txt + '[' + ''.join(['-']*dash_len)+']'\n",
    "    stdout.write('\\r')\n",
    "    f = f.replace('-', '#', int((iter+1)/max*dash_len))\n",
    "    f += f'{(iter+1)/max * 100:.2f} ### {time.time()-start_time:.1f} elapsed.'\n",
    "    stdout.write(f)\n",
    "    stdout.flush()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, device, test_dataloader, criterion, threshold = .5):\n",
    "    model.eval()\n",
    "    dsize = len(test_dataloader)\n",
    "    cm = None\n",
    "    outer_loss = 0\n",
    "    start_time = time.time()\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        outer_loss += criterion(output, labels).cpu().item()\n",
    "        predict = (output >= threshold).float()\n",
    "        ccm = ConfuseMatrix(labels.cpu().numpy(), predict.cpu().numpy())\n",
    "        progressbar(dsize, i, start_time, 10)\n",
    "    cm += ccm\n",
    "    return outer_loss, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper parameters\n",
    "epoch = 100\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "embed_size = 160\n",
    "att_head = 8\n",
    "mlp_size = embed_size\n",
    "dropout = 0\n",
    "tblock = 2\n",
    "\n",
    "sf = f'./models/ViT_{epoch}_{img_size}_{patch_size}_{embed_size}_{att_head}_{mlp_size}_{tblock}_{dropout}.pth'\n",
    "sf_best = f'./models/ViT_{epoch}_{img_size}_{patch_size}_{embed_size}_{att_head}_{mlp_size}_{tblock}_{dropout}_best.pth'\n",
    "assert os.path.exists(sf) == True\n",
    "assert os.path.exists(sf_best) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(img_size, patch_size, 3, 8, embed_size, att_head, mlp_size, tblock, dropout*.1)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename=\"model_checkpoint.pth\", device = 'cuda'):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(filename, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "epoch = 100, loss = 11.28\n",
      "[##########]100.00 ### 7.8 elapsed.<torch.utils.data.dataloader.DataLoader object at 0x000001F10C464310>\n",
      "\n",
      "accuracy =  0.4482757074911354\n",
      "loss = 2.15\n",
      "specificity =  [0.29 0.82 1.   0.93 1.   1.   0.92 1.  ]\n",
      "sensitivity =  [0.8  0.57 0.   0.   0.   0.   0.5  0.  ]\n",
      "precision =  [0.4  0.67 0.   0.   0.   0.   0.5  0.  ]\n"
     ]
    }
   ],
   "source": [
    "epoch, loss = load_checkpoint(sf)\n",
    "print(f\"epoch = {epoch}, loss = {loss:.2f}\")\n",
    "loss, cm = test(model, device, test_dataloader, criterion)\n",
    "print(test_dataloader)\n",
    "\n",
    "print('\\naccuracy = ', cm.acc())\n",
    "\n",
    "print(f'loss = {loss:.2f}')\n",
    "\n",
    "print('specificity = ', cm.spec().round(2))\n",
    "\n",
    "print('sensitivity = ', cm.sens().round(2))\n",
    "\n",
    "print('precision = ', cm.prec().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "epoch = 80, loss = 11.82\n",
      "[##########]100.00 ### 7.7 elapsed.<torch.utils.data.dataloader.DataLoader object at 0x000001F10C464310>\n",
      "\n",
      "accuracy =  0.35483859521335637\n",
      "loss = 2.13\n",
      "specificity =  [0.33 0.43 1.   0.92 1.   1.   0.92 1.  ]\n",
      "sensitivity =  [0.46 0.71 0.   0.   0.   0.   0.   0.  ]\n",
      "precision =  [0.37 0.38 0.   0.   0.   0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "epoch, loss = load_checkpoint(sf_best)\n",
    "print(f\"epoch = {epoch}, loss = {loss:.2f}\")\n",
    "loss, cm = test(model, device, test_dataloader, criterion)\n",
    "print(test_dataloader)\n",
    "\n",
    "print('\\naccuracy = ', cm.acc())\n",
    "\n",
    "print(f'loss = {loss:.2f}')\n",
    "\n",
    "print('specificity = ', cm.spec().round(2))\n",
    "\n",
    "print('sensitivity = ', cm.sens().round(2))\n",
    "\n",
    "print('precision = ', cm.prec().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
